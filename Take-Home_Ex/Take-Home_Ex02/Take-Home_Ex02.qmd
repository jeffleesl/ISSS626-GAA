---
title: "Take Home Exercise 2 : Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics"
author: "Jeffrey Lee Shao Lin"
date: "September 27, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  freeze: true
  warning: false
---

# 1. Setting the Scene

Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US\$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US\$ in 2020.

Figure below shows the total revenue receipt from tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue from tourism industry have been recovered gradually since September 2021.

![](images/clipboard-378385426.png)

However, it is important to note that the tourism economy of Thailand are not evenly distributed. Figure below reveals that the tourism economy of Thailand are mainly focus on five provinces, namely Bangkok, Phuket, Chiang Mai, Sukhothai and Phetchaburi.

![](images/clipboard-986836251.png)

# 2. Objectives

As a a curious geospatial analytics green horn, we are interested to discover:

-   if the key indicators of tourism economy of Thailand are independent from space and space and time.

-   If the tourism economy is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

# 3. The Task

The specific tasks of this take-home exercise are as follows:

Using appropriate function of sf and tidyverse, preparing the following geospatial data layer: a study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand. a tourism economy indicators layer within the study area in sf polygon features. a derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels. Using the extracted data, perform global spatial autocorrelation analysis by using sfdep methods. Using the extracted data, perform local spatial autocorrelation analysis by using sfdep methods. Using the extracted data, perform emerging hotspot analysis by using sfdep methods. Describe the spatial patterns revealed by the analysis above.

# 4. The Data

For the purpose of this take-home exercise, two data sets shall be used, they are:

Thailand Domestic Tourism Statistics at Kaggle. You are required to use version 2 of the data set.

Thailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set.

# 5. Importing Packages

Before we start the exercise, we will need to import necessary R packages first. We will use the following packages:

-   [sf](https://r-spatial.github.io/sf/index.html) package provides functions to manage, processing, and manipulate **Simple Features**, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html) which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using [leaflet](https://leafletjs.com/) API.

-   [**sfdep**](https://cran.r-project.org/web/packages/sfdep/index.html) which provide functions for utilizes list columns extensively to make this interface possible.

-   [**smoothr**](https://cran.r-project.org/web/packages/smoothr/index.html) which provide functions for smoothing and tidying spatial features (i.e. lines and polygons) to make them more aesthetically pleasing.

-   [**lubridate**](https://cran.r-project.org/web/packages/lubridate/index.html) which provide functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects.

-   [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/) which create advanced visualisations, graphics and maps using the Grammar of Graphics.

-   [`Kendall`](https://cran.r-project.org/web/packages/Kendall/) which compute the Kendall rank correlation and Mann-Kendall trend test.

    Use the code chunk below to install and launch the below R packages.

```{r}
pacman::p_load(sf, sfdep, tmap, lubridate, plotly, tidyverse, ggplot2, Kendall)
```

# 6. Getting the Data Into R Environment

In this section, we will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.

## 6.1 Import shapefile into r environment

### 6.1.1 Thailand Domestic Tourism Statistics

```{r}
tourism_data <- read_csv("data/rawdata/thailand_domestic_tourism_2019_2023_ver2.csv") 
# Update with actual path
```

```{r}
glimpse(tourism_data)
```

Variables:

|                      |                                                                             |
|------------------------|-----------------------------------------------|
| `no_tourist_all`     | The total number of domestic tourists who visited the province              |
| `no_tourist_foreign` | The number of foreign tourists who visited the province                     |
| `no_tourist_stay`    | The number of tourists who stay over-night                                  |
| `no_tourist_thai`    | The number of Thai tourists who visited the province                        |
| `ratio_tourist_stay` | The ratio of tourist stay over-night.                                       |
| `revenue_all`        | The revenue generated by the tourism industry in the province, in Thai Baht |
| `revenue_foreign`    | The revenue generated by foreign tourists in the province, in Thai Baht     |
| `revenue_thai`       | The revenue generated by Thai tourists in the province, in Thai Baht        |

#### 6.1.1.2 Reshape the Data

Use pivot_wider to create new columns based on the variables in Column F, using Column G as the values.

```{r}
tourism_data_wide <- tourism_data %>%
  pivot_wider(
    names_from = variable,
    values_from = value,
    values_fill = NA  # Fill missing values with NA 
    )  #%>% 
  #filter(!is.na(revenue_all) & revenue_all != 0)

tourism_data_wide
```

#### 6.1.1.2 Select Relevant Columns

After reshaping, select only the relevant columns for your analysis. Also, create new columns for month, month factor and month-year.

```{r}
tourism_data_new <- tourism_data_wide %>%
  select(date, province_thai, province_eng, region_eng, 
         no_tourist_all, no_tourist_foreign, 
         no_tourist_stay, no_tourist_thai, 
         ratio_tourist_stay, revenue_all, 
         revenue_foreign, revenue_thai) %>%
  mutate(
    Month_num = month(date),  # Extract numeric month
    Month_fac = month(date, label = TRUE, abbr = TRUE),  # Extract abbreviated month as factor
    Month_year = paste0(Month_fac, "-", year(date))  # Create 'month-year' column
  )

print(tourism_data_new)
```

### 6.1.2 Thailand - Subnational Administrative Boundaries

```{r}
# Load province boundaries

provinces <- st_read(dsn = "data/rawdata", 
                layer = "tha_admbnda_adm1_rtsd_20220121") %>%
  st_transform(crs = 32647) %>%
  mutate(ADM1_EN = recode(ADM1_EN, # Update province boundaries' name
    "Lop Buri" = "Lopburi",
    "Chai Nat" = "Chainat",
    "Chon Buri" = "Chonburi",
    "Prachin Buri" = "Prachinburi",
    "Phangnga" = "Phang Nga",
    "Buri Ram" = "Buriram",
    "Si Sa Ket" = "Sisaket",
    "Nong Bua Lam Phu" = "Nong Bua Lamphu"
  ))

```

```{r}
# Load province boundaries

glimpse(provinces)

```

```{r}
tm_shape(provinces) +
  tm_polygons()

```

```{r}
## Convert to multipolygon to individual polygon
provinces_sf <- provinces %>% 
  st_cast("POLYGON") %>% 
  mutate(area = st_area(.))
```

```{r}
## Group by the unique name and select the largest polygon by area
provinces_cleaned <- provinces_sf %>% 
  group_by(ADM1_EN) %>% 
  filter(area == max(area)) %>% 
  ungroup() %>% 
  select(-area) %>% 
  select(ADM1_EN)
```

## 6.2 Performing relational join

The code chunk below will be used to update the attribute table of *provinces*’ SpatialPolygonsDataFrame with the attribute fields of *tourismdatanew* dataframe. This is performed by using *left_join()* of **dplyr** package.

### 6.2.1 Using Province in English

```{r}
# Left join to add geometries from thailand bundaries shapefile
tourism_sf <- tourism_data_new %>%
  left_join(provinces_cleaned, by = c("province_eng" = "ADM1_EN")) 

# Ensure the data is a valid sf object
tourism_sf <- st_as_sf(tourism_sf)

# Check if transformation was successful
st_crs(tourism_sf)
```

### 6.2.2 Using Province in Thai (Showing Other Method)

```{r}
# Left join to add geometries from thailand bundaries shapefile
# tourism_sf_th <- tourism_data_new %>%
  # left_join(provinces, by = c("province_thai" = "ADM1_TH"))

# Ensure the data is a valid sf object
#tourism_sf_th <- st_as_sf(tourism_sf_th)

# Check if transformation was successful
# st_crs(tourism_sf_th)
```

### 6.2.3 Tourism Data and Information

::: panel-tabset
## The total number of domestic tourists who visited the province

```{r}
no_tourist_all_sf <- tourism_sf %>%
  select(1:5, 15:16)
```

## The number of foreign tourists who visited the province

```{r}
no_tourist_foreign_sf <- tourism_sf %>%
  select(1:4, 6, 15:16)
```

## The number of tourists who stay over-night

```{r}
no_tourist_stay_sf <- tourism_sf %>%
  select(1:4, 7, 15:16)
```

## The number of Thai tourists who visited the province

```{r}
no_tourist_thai_sf <- tourism_sf %>%
  select(1:4, 8, 15:16)
```

## The ratio of tourist stay over-night

```{r}
ratio_tourist_stay_sf <- tourism_sf %>%
  select(1:4, 9, 15:16)
```

## The revenue generated by the tourism industry in the province, in Thai Baht

```{r}
revenue_all_sf <- tourism_sf %>%
  select(1:4, 10, 15:16)
```

## The revenue generated by foreign tourists in the province, in Thai Baht

```{r}
revenue_foreign_sf <- tourism_sf %>%
  select(1:4, 11, 15:16)
```

## The revenue generated by Thai tourists in the province, in Thai Baht

```{r}
revenue_thai_sf <- tourism_sf %>%
  select(1:4, 15:16)
```
:::

# 7.Derived Tourism Economy Indicator Layer

It is always a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

## 7.1 Plotting a choropleth map

Plot a choropleth map showing the distribution of revenue generated by the tourism industry in the different province, in Thai Baht

```{r}
tmap_mode("plot")
tm_shape(tourism_sf) +
  tm_fill("revenue_all",
          style = "quantile",
          palette = "Blues",
          title = "Total Revenue") +
  tm_borders(col = "grey") +
  tm_facets("Month_year") +
  tm_layout(main.title = "Distribution of revenue generated by the tourism industry in the different province",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45,
            legend.width = 0.35,
            frame = TRUE)
```

### 7.1.1 Calculate the Average Revenue for Each Province

```{r}
# Calculate the average revenue_all for each province
tourism_sf_avg <- tourism_sf %>%
  group_by(province_thai) %>%
  summarize(average_revenue_all = mean(revenue_all, na.rm = TRUE))

```

### 7.1.2 Join the Average Revenue Back to the Spatial Data

```{r}
# Join the average revenue back to the spatial data
tourism_sf_avg <- left_join(st_drop_geometry(tourism_sf), tourism_sf_avg, by = "province_thai")
tourism_sf_avg <- st_as_sf(tourism_sf_avg)

```

### 7.1.3 Modify the Plot Code to Use average_revenue_all

```{r}
set.seed(1234)
tmap_mode("plot")
tm_shape(tourism_sf_avg) +
  tm_fill("average_revenue_all",   # Use average revenue
          style = "quantile",      # Use quantile classification
          palette = "Blues",       # Choose color palette
          title = "Average Revenue (All)") +
  tm_borders(col = "grey") +
  tm_facets("Month_year") +
  tm_layout(main.title = "Average Revenue Generated by Tourism Industry in Different Provinces",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45,
            legend.width = 0.35,
            frame = TRUE)

```

## 7.2 Visualising Regional Development Indicator

Now, we are going to prepare a basemap and a choropleth map showing the distribution of the revenue by using qtm() of tmap package.

### 7.2.1 Equal interval classification and Equal quantile classification

```{r}
set.seed(1234)

# Equal interval classification
equal <- tm_shape(tourism_sf) +
  tm_fill("revenue_all",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_facets("Month_year") +
  tm_layout(main.title = "Equal interval classification",
            legend.width = 1.2)  # Increase the legend width

# Quantile classification
quantile <- tm_shape(tourism_sf) +
  tm_fill("revenue_all",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_facets("Month_year") +
  tm_layout(main.title = "Equal quantile classification",
            legend.width = 1.2)  # Increase the legend width

# Arrange the two maps side by side
tmap_arrange(equal, 
             quantile, 
             asp = 1, 
             ncol = 2)
```

### 7.2.2 Equal interval classification Y2023 and Equal quantile classification Y2023

```{r}
set.seed(1234)
tourism_sf_2023 <- tourism_sf %>%
  filter(year(date) == 2023)

equal_2023 <- tm_shape(tourism_sf_2023) +
  tm_fill("revenue_all",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_facets("Month_year") +
  tm_layout(main.title = "Equal interval classification Y2023",
            legend.width = 1.2)

quantile_2023 <- tm_shape(tourism_sf_2023) +
  tm_fill("revenue_all",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_facets("Month_year") +
  tm_layout(main.title = "Equal quantile classification Y2023",
            legend.width = 1.2)

tmap_arrange(equal_2023, 
             quantile_2023, 
             asp=1, 
             ncol=2)
```

# 8. Creating a Time Series Cube

```{r}
# Load province boundaries
provinces_new <- provinces %>%
  rename(province_eng = ADM1_EN)
```

```{r}
tourism_st <- spacetime(tourism_sf, provinces_new,
                       .loc_col = "province_eng",
                        .time_col = "date")
```

```{r}
is_spacetime_cube(tourism_st)
```

The TRUE return confirms that GDPPC_st object is indeed an time-space cube.

# 9. Global Spatial Autocorrelation Analysis

## 9.1 Deriving Queen’s contiguity weights: sfdep methods

```{r}
#wm_q <- tourism_st %>%
  #mutate(nb = st_contiguity(geometry),
         #wt = st_weights(nb, style = "W"),
         #.before = 1)
```

```{r}
#wm_q <- tourism_sf %>%
  #mutate(nb = st_contiguity(geometry),
       #  wt = st_weights(nb, style = "W")) %>%
#  select(nb, wt, everything()) 
```


```{r}
#tourism_q <- st_contiguity (tourism_sf, queen=TRUE)
#summary(wm_q)
```

```{r}
#tourism_wm_rs <- st_weights(tourism_q, style="W")
```

```{r}
#wm_q <- tourism_sf %>%
 # mutate(nb = tourism_q,
   #      wt = tourism_wm_rs,
   #      .before = 1) 
```

```{r}
#wm_q

```

## 9.2 Computing Global Moran’ I

In the code chunk below, global_moran() function is used to compute the Moran’s I value. Different from spdep package, the output is a tibble data.frame.

```{r}
#moranI <- global_moran(wm_q$revenue_all,
      #                 wm_q$nb,
      #                 wm_q$wt)
#glimpse(moranI)

```

## 9.3 Performing Global Moran’sI test

In general, Moran’s I test will be performed instead of just computing the Moran’s I statistics. With sfdep package, Moran’s I test can be performed by using global_moran_test() as shown in the code chunk below.

```{r}
#global_moran_test(wm_q$revenue_all,
            #      wm_q$nb,
           #       wm_q$wt)
```

## 9.4 Global Moran’I permutation test

Next, global_moran_perm() is used to perform Monte Carlo simulation.

```{r}
#global_moran_perm(wm_q$revenue_all,
             #     wm_q$nb,
             #     wm_q$wt,
             #     nsim =99)
```

The statistical report on previous tab shows that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of total revenue are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.

```{r}
# Global Moran's I for tourism data
#global_moran_test <- tourism_sf %>%
  #global_moran(variable = "no_tourist_all", nb = tourism_neighbors)

# Print the results
#global_moran_test

```

# 10. Local Spatial Autocorrelation Analysis

## 10.1 Computing local Moran’s I

In this section, we compute Local Moran’s I of total revenue at province level by using local_moran() of sfdep package.

```{r}
#lisa <- wm_q %>%
 # mutate(local_moran = local_moran(
  #  revenue_all, nb, wt, nsim = 99),
  #  .before = 1) %>%
 # unnest(local_moran)

```

## 10.2 Visualising local Moran’s I

In this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.

```{r}
#tmap_mode("plot")
#tm_shape(lisa) +
 # tm_fill("ii") +
  #tm_borders(alpha = 0.5) +
 # tm_facets("Month_year") +
 # tm_view(set.zoom.limits = c(6,8)) +
 # tm_layout(
  #  main.title = "local Moran's I of Total Revenue",
  #  main.title.size = 2)
```

## 10.3 Visualising p-value of local Moran’s I

```{r}
#tmap_mode("plot")
#tm_shape(lisa) +
  #tm_fill("p_ii_sim") +
  #tm_borders(alpha = 0.5) +
  #tm_facets("Month_year") +
  #tm_layout(
  #  main.title = "p-values of local Moran's I",
  #  main.title.size = 2)
```

## 10.4 Visualising local Moran’s I and p-value

```{r}
#tmap_mode("plot")
#map1 <- tm_shape(lisa) +
#  tm_fill("ii") + 
 # tm_borders(alpha = 0.5) +
 # tm_facets("Month_year") +
 # tm_view(set.zoom.limits = c(6,8)) +
 # tm_layout(main.title = "local Moran's I of Total Revenue",
   #         main.title.size = 0.8)

#map2 <- tm_shape(lisa) +
  #tm_fill("p_ii",
   #       breaks = c(0, 0.001, 0.01, 0.05, 1),
    #          labels = c("0.001", "0.01", "0.05", "Not sig")) + 
 # tm_borders(alpha = 0.5) +
#  tm_facets("Month_year") +
#  tm_layout(main.title = "p-value of local Moran's I",
         #   main.title.size = 0.8)

#tmap_arrange(map1, map2, ncol = 2)

```

## 10.5 Plotting LISA map

```{r}
#lisa_sig <- lisa %>%
 # filter(p_ii < 0.05) #filter only significant p values

#tmap_mode("plot")
#tm_shape(lisa)+
 # tm_polygons()+
 # tm_borders(alpha=0.5)+
#tm_shape(lisa_sig)+
#  tm_fill("mean")+
#  tm_borders(alpha=0.4)

```

## 10.6 Computing local Gi\* statistics

As usual, we will need to derive a spatial weight matrix before we can compute local Gi\* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.

```{r}
#wm_idw <- revenue_all_sf %>% 
 # mutate(nb = st_contiguity(geometry) ,
     #    wts = st_inverse_distance(nb, geometry,
       #                           scale = 1,
       #                           alpha =1),
       #  .before = 1)

```

```{r}
#wm_idw <- revenue_all_sf %>%
 # mutate(nb = include_self(
  #  st_contiguity(geometry)),
  #  wts = st_inverse_distance(nb, 
        #                      geometry, 
        #                      scale = 1,
        #                      alpha = 1),
        # .before = 1)
```

Now, we will compute the local Gi\* by using the code chunk below.

```{r}
# HCSA <- wm_idw %>% 
  #mutate(local_Gi = local_gstar_perm(
   #revenue_all, nb, wts, nsim = 99),
    #     .before = 1) %>%
 # unnest(local_Gi)
#HCSA
```

### 10.6.1 Visualising Gi\*

In the code chunk below, tmap functions are used to plot the local Gi\* (i.e. gi_star) at the province level.

```{r}
#tmap_mode("plot")
#tm_shape(HCSA)+
#  tm_fill("gi_star")+
#  tm_borders(alpha = 0.5) +
 # tm_view(set.zoom.limits = c(6,8))
```

### 10.6.2 Visualising p-value of HCSA

In the code chunk below, tmap functions are used to plot the p-values of local Gi\* (i.e. p_sim) at the province level.

```{r}
#tmap_mode("plot")
#tm_shape(HCSA) +
 # tm_fill("p_sim") + 
 # tm_borders(alpha = 0.5)
```

### 10.6.3 Visuaising local HCSA

```{r}
# tmap_mode("plot")
#map1 <- tm_shape(HCSA) +
#  tm_fill("gi_star") + 
 # tm_borders(alpha = 0.5) +
 # tm_view(set.zoom.limits = c(6,8)) +
 # tm_layout(main.title = "Gi* of GDPPC",
   #         main.title.size = 0.8)

#map2 <- tm_shape(HCSA) +
 # tm_fill("p_value",
  #        breaks = c(0, 0.001, 0.01, 0.05, 1),
     #         labels = c("0.001", "0.01", "0.05", "Not sig")) + 
#  tm_borders(alpha = 0.5) +
#  tm_layout(main.title = "p-value of Gi*",
   #         main.title.size = 0.8)

#tmap_arrange(map1, map2, ncol = 2)
```

### 10.6.4 Visualising hot spot and cold spot areas

```{r}
#HCSA_sig <- HCSA  %>%
 # filter(p_sim < 0.05)
#tmap_mode("plot")
#tm_shape(HCSA) +
 # tm_polygons() +
 # tm_borders(alpha = 0.5) +
#tm_shape(HCSA_sig) +
 # tm_fill("cluster") + 
  #tm_borders(alpha = 0.4)
```

Figure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section.

# 11. Emerging Hotspot Analysis

## 11.1 Computing Gi\*

The code chunk below will be used to identify neighbors and to derive an inverse distance weights.

```{r}
#tourism_nb <- tourism_st %>% 
#  activate("geometry") %>% # activate the geometry context
 # mutate(nb = include_self( #mutate to create two new columns nb and wt, include itself
  #  st_contiguity(geometry)),
  #  wt = st_inverse_distance(nb,
     #                        geometry,
     #                        scale = 1,
      #                       alpha = 1),
  #  .before = 1) %>% #new derived variable in front of the table
  #set_nbs("nb") %>% 
  #set_wts("wt")
```

We can use these new columns to manually calculate the local Gi\* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.

```{r}
#gi_stars <- tourism_nb %>% 
 # group_by(Year) %>% 
 # mutate(gi_star = local_gstar_perm(
 #   revenue_all, nb, wt)) %>% 
 # tidyr::unnest(gi_star)
```

## 11.2 Mann-Kendall Test of GI

With these Gi\* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.

A **monotonic series** or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.

H0: No monotonic trend

H1: Monotonic trend is present

**Interpretation**

-   Reject the null-hypothesis null if the p-value is smaller than the alpha value (i.e. 1-confident level)

-   Tau ranges between -1 and 1 where:

    -   -1 is a perfectly decreasing series, and

    -   1 is a perfectly increasing series.

```{r}
#cbg <- gi_stars %>% 
 # ungroup () %>% 
 # filter(province_eng == "Bangkok") %>%
 # select(province_eng, Year, gi_star)
```

Next, we plot the result by using ggplot2 functions.

```{r}
#ggplot(data = cbg,
    #   aes(x = Year,
    #       y = gi_star)) +
#  geom_line() +
#  theme_light()
```

## 11.3 Interactive Mann-Kendall Plot

We can also create an interactive plot by using ggplotly() of plotly package.

```{r}
#gp <- ggplot(data = cbg,
 #           aes(x = Year,
  #              y = gi_star))+
 # geom_line()+
#  theme_light()

#ggplotly(p)
```

## 11.4 Printing Mann-Kendall test report

```{r}
#cbg %>% 
 # summarise(mk = list(
  #  unclass(
  #    Kendall::MannKendall(gi_star)))) %>% 
 # tidyr::unnest_wider(mk)
```

In the above result, sl is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.

## 11.5 Mann-Kendall test data.frame

We can replicate this for each location by using group_by() of dplyr package.

```{r}
#ehsa <- gi_stars %>% 
 # group_by(province_eng) %>% 
 # summarise(mk = list(
 #   unclass(
 #     Kendall::MannKendall(gi_star)))) %>% 
 # tidyr::unnest_wider(mk)
# head(ehsa)
```

We can also sort to show significant emerging hot/cold spots

```{r}
#emerging <- ehsa %>% 
 # arrange(sl, abs(tau)) %>% 
 # slice(1:10)
#head(emerging)
```

## 11.6 Performing Emerging Hotspot Analysis

Lastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. Total Revenue) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.

```{r}
#ehsa <- emerging_hotspot_analysis(
 # x = tourism_st,
 # .var = "GDPPC",
#  k = 1,
#  nsim = 99
#)
```

## 11.7 Visualising the distribution of EHSA classes

In the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.

```{r}
#ggplot(data = ehsa,
 #      aes(x = classification)) +
 # geom_bar()
```

Figure above shows that sporadic cold spots class has the high numbers of county.

## 11.8 Visualising EHSA

```{r}
#tourism_ehsa <- tourism_st %>% 
#  left_join(ehsa,
  #          by = join_by(province_ == location))
```

Next, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.

```{r}
#tourism_sig <- tourism_ehsa %>%
#  filter(p_value < 0.05)
#tmap_mode("plot")
#tm_shape(hunan_ehsa) +
#  tm_polygons()+
#  tm_borders(alpha = 0.5) +
#tm_shape(ehsa_sig)+
#  tm_fill("classification")+
#  tm_borders(alpha = 0.4)
```

# 12. Conclusion

-   **Global Autocorrelation**: A significant positive Moran's I indicates clustering of high or low values.

-   **Local Autocorrelation**: Identify specific provinces with high or low values using LISA results.

-   **Hotspots**: Areas with high positive Gi\* scores are emerging hotspots, while negative scores indicate cold spots.

This framework allows us to explore spatial dependencies in Thailand's tourism economy effectively. Adjust paths and variable names according to your specific datasets.

Output is saved in rds format for future used.

```{r}
write_rds(tourism_sf,"data/rds/tourism.rds")
```

The code chunk below will be used to import the save origin6_9.rds into R environment.

```{r}
acc <- read_rds("data/rds/tourism.rds")
```
